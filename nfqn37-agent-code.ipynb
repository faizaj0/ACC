{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn import Module, Linear\n",
    "from torch.distributions import Distribution, Normal\n",
    "from torch.nn.functional import relu, logsigmoid\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "from pyvirtualdisplay import Display\n",
    "from IPython import display as disp\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaptively Calibrated Critics (ACC)\n",
    "\n",
    "Implementation of the Adaptively Calibrated Critics (ACC) algorithm, an deep RL approach designed to dynamically adjust bias in Temporal Difference (TD) learning targets. \n",
    "\n",
    "ACC builds upon the concepts of Truncated Quantile Critics (TQC) by introducing dynamic adjustments to the quantile targets exclusion parameter \\( \\beta \\), improving the precision in bias adjustment. This allows for a more nuanced control between over- and underestimation, thereby improving both the robustness and accuracy of the policy learning process.\n",
    "\n",
    "## Source\n",
    "The code is heavily adapted from the original ACC implementation available at [Nicolinho/ACC](https://github.com/Nicolinho/ACC/tree/main)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "EPISODE_LENGTH = 2000\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "LOG_STD_MIN_MAX = (-20, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class RescaleAction(gym.ActionWrapper):\n",
    "    def __init__(self, env, a, b):\n",
    "        assert isinstance(env.action_space, spaces.Box), (\n",
    "            \"expected Box action space, got {}\".format(type(env.action_space)))\n",
    "        assert np.less_equal(a, b).all(), (a, b)\n",
    "        super(RescaleAction, self).__init__(env)\n",
    "        self.a = np.zeros(env.action_space.shape, dtype=env.action_space.dtype) + a\n",
    "        self.b = np.zeros(env.action_space.shape, dtype=env.action_space.dtype) + b\n",
    "        self.action_space = spaces.Box(low=a, high=b, shape=env.action_space.shape, dtype=env.action_space.dtype)\n",
    "\n",
    "    def action(self, action):\n",
    "        assert np.all(np.greater_equal(action, self.a)), (action, self.a)\n",
    "        assert np.all(np.less_equal(action, self.b)), (action, self.b)\n",
    "        low = self.env.action_space.low\n",
    "        high = self.env.action_space.high\n",
    "        action = low + (high - low)*((action - self.a)/(self.b - self.a))\n",
    "        action = np.clip(action, low, high)\n",
    "        return action\n",
    "\n",
    "class Mlp(Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_size,\n",
    "            hidden_sizes,\n",
    "            output_size\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.fcs = []\n",
    "        in_size = input_size\n",
    "        for i, next_size in enumerate(hidden_sizes):\n",
    "            fc = Linear(in_size, next_size)\n",
    "            self.add_module(f'fc{i}', fc)\n",
    "            self.fcs.append(fc)\n",
    "            in_size = next_size\n",
    "        self.last_fc = Linear(in_size, output_size)\n",
    "\n",
    "    def forward(self, input):\n",
    "        h = input\n",
    "        for fc in self.fcs:\n",
    "            h = relu(fc(h))\n",
    "        output = self.last_fc(h)\n",
    "        return output\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "    def __init__(self, state_dim, action_dim, max_size=int(1e6)):\n",
    "        self.max_size = max_size\n",
    "        self.ptr = 0\n",
    "        self.size = 0\n",
    "\n",
    "        self.transition_names = ('state', 'action', 'next_state', 'reward', 'not_done')\n",
    "        sizes = (state_dim, action_dim, state_dim, 1, 1)\n",
    "        for name, size in zip(self.transition_names, sizes):\n",
    "            setattr(self, name, np.empty((max_size, size)))\n",
    "\n",
    "    def add(self, state, action, next_state, reward, done):\n",
    "        values = (state, action, next_state, reward, 1. - done)\n",
    "        for name, value in zip(self.transition_names, values):\n",
    "            getattr(self, name)[self.ptr] = value\n",
    "\n",
    "        self.ptr = (self.ptr + 1) % self.max_size\n",
    "        self.size = min(self.size + 1, self.max_size)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        ind = np.random.randint(0, self.size, size=batch_size)\n",
    "        names = self.transition_names\n",
    "        return (torch.FloatTensor(getattr(self, name)[ind]).to(DEVICE) for name in names)\n",
    "\n",
    "    def states_by_ptr(self, ptr_list, cpu=False):\n",
    "        ind = np.array([], dtype='int64')\n",
    "        for interval in ptr_list:\n",
    "            if interval[0] < interval[1]:\n",
    "                ind = np.concatenate((ind, np.arange(interval[0], interval[1])))\n",
    "            elif interval[0] > interval[1]:\n",
    "                ind = np.concatenate((ind, np.arange(interval[0], self.max_size)))\n",
    "                ind = np.concatenate((ind, np.arange(0, interval[1])))\n",
    "\n",
    "        names = ('state', 'action')\n",
    "        if cpu:\n",
    "            return (torch.FloatTensor(getattr(self, name)[ind]) for name in names)\n",
    "        else:\n",
    "            return (torch.FloatTensor(getattr(self, name)[ind]).to(DEVICE) for name in names)\n",
    "\n",
    "\n",
    "class Critic(Module):\n",
    "    def __init__(self, state_dim, action_dim, n_quantiles, n_nets):\n",
    "        super().__init__()\n",
    "        self.nets = []\n",
    "        self.n_quantiles = n_quantiles\n",
    "        self.n_nets = n_nets\n",
    "        for i in range(n_nets):\n",
    "            net = Mlp(state_dim + action_dim, [512, 512, 512], n_quantiles)\n",
    "            self.add_module(f'qf{i}', net)\n",
    "            self.nets.append(net)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        sa = torch.cat((state, action), dim=1)\n",
    "        quantiles = torch.stack(tuple(net(sa) for net in self.nets), dim=1)\n",
    "        return quantiles\n",
    "\n",
    "\n",
    "class Actor(Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.action_dim = action_dim\n",
    "        self.net = Mlp(state_dim, [256, 256], 2 * action_dim)\n",
    "\n",
    "    def forward(self, obs):\n",
    "        mean, log_std = self.net(obs).split([self.action_dim, self.action_dim], dim=1)\n",
    "        log_std = log_std.clamp(*LOG_STD_MIN_MAX)\n",
    "\n",
    "        if self.training:\n",
    "            std = torch.exp(log_std)\n",
    "            tanh_normal = TanhNormal(mean, std)\n",
    "            action, pre_tanh = tanh_normal.rsample()\n",
    "            log_prob = tanh_normal.log_prob(pre_tanh)\n",
    "            log_prob = log_prob.sum(dim=1, keepdim=True)\n",
    "        else:  # deterministic eval without log_prob computation\n",
    "            action = torch.tanh(mean)\n",
    "            log_prob = None\n",
    "        return action, log_prob\n",
    "\n",
    "    def select_action(self, obs):\n",
    "        obs = torch.FloatTensor(obs).to(DEVICE)[None, :]\n",
    "        action, _ = self.forward(obs)\n",
    "        action = action[0].cpu().detach().numpy()\n",
    "        return action\n",
    "\n",
    "\n",
    "class TanhNormal(Distribution):\n",
    "    def __init__(self, normal_mean, normal_std):\n",
    "        super().__init__()\n",
    "        self.normal_mean = normal_mean\n",
    "        self.normal_std = normal_std\n",
    "        self.standard_normal = Normal(torch.zeros_like(self.normal_mean, device=DEVICE),torch.ones_like(self.normal_std, device=DEVICE))\n",
    "        self.normal = Normal(normal_mean, normal_std)\n",
    "\n",
    "    def log_prob(self, pre_tanh):\n",
    "        log_det = 2 * np.log(2) + logsigmoid(2 * pre_tanh) + logsigmoid(-2 * pre_tanh)\n",
    "        result = self.normal.log_prob(pre_tanh) - log_det\n",
    "        return result\n",
    "\n",
    "    def rsample(self):\n",
    "        pretanh = self.normal_mean + self.normal_std * self.standard_normal.sample()\n",
    "        return torch.tanh(pretanh), pretanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_policy(policy, eval_env, max_episode_steps, eval_episodes=10):\n",
    "    policy.eval()\n",
    "    avg_reward = 0.\n",
    "    for _ in range(eval_episodes):\n",
    "        state, done = eval_env.reset(), False\n",
    "        t = 0\n",
    "        while not done and t < max_episode_steps:\n",
    "            action = policy.select_action(state)\n",
    "            state, reward, done, _ = eval_env.step(action)\n",
    "            avg_reward += reward\n",
    "            t += 1\n",
    "    avg_reward /= eval_episodes\n",
    "    policy.train()\n",
    "    return avg_reward\n",
    "\n",
    "\n",
    "def quantile_huber_loss_f(quantiles, samples):\n",
    "    pairwise_delta = samples[:, None, None, :] - quantiles[:, :, :, None]  # batch x nets x quantiles x samples\n",
    "    abs_pairwise_delta = torch.abs(pairwise_delta)\n",
    "    huber_loss = torch.where(abs_pairwise_delta > 1,\n",
    "                             abs_pairwise_delta - 0.5,\n",
    "                             pairwise_delta ** 2 * 0.5)\n",
    "\n",
    "    n_quantiles = quantiles.shape[2]\n",
    "    tau = torch.arange(n_quantiles, device=DEVICE).float() / n_quantiles + 1 / 2 / n_quantiles\n",
    "    loss = (torch.abs(tau[None, None, :, None] - (pairwise_delta < 0).float()) * huber_loss).mean()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "    def __init__(\n",
    "            self,\n",
    "            *,\n",
    "            actor,\n",
    "            critic,\n",
    "            critic_target,\n",
    "            discount,\n",
    "            tau,\n",
    "            top_quantiles_to_drop,\n",
    "            target_entropy,\n",
    "            use_acc,\n",
    "            lr_dropped_quantiles,\n",
    "            adjusted_dropped_quantiles_init,\n",
    "            adjusted_dropped_quantiles_max,\n",
    "            diff_ma_coef,\n",
    "            num_critic_updates,\n",
    "            # writer\n",
    "    ):\n",
    "\n",
    "        self.actor = actor\n",
    "        self.critic = critic\n",
    "        self.critic_target = critic_target\n",
    "        self.log_alpha = torch.zeros((1,), requires_grad=True, device=DEVICE)\n",
    "\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=3e-4)\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=3e-4)\n",
    "        self.alpha_optimizer = torch.optim.Adam([self.log_alpha], lr=3e-4)\n",
    "\n",
    "        self.discount = discount\n",
    "        self.tau = tau\n",
    "        self.top_quantiles_to_drop = top_quantiles_to_drop\n",
    "        self.target_entropy = target_entropy\n",
    "\n",
    "        self.quantiles_total = critic.n_quantiles * critic.n_nets\n",
    "\n",
    "        self.total_it = 0\n",
    "\n",
    "        # self.writer = writer\n",
    "\n",
    "        self.use_acc = use_acc\n",
    "        self.num_critic_updates = num_critic_updates\n",
    "        if use_acc:\n",
    "            self.adjusted_dropped_quantiles = torch.tensor(adjusted_dropped_quantiles_init, requires_grad=True)\n",
    "            self.adjusted_dropped_quantiles_max = adjusted_dropped_quantiles_max\n",
    "            self.dropped_quantiles_dropped_optimizer = torch.optim.SGD([self.adjusted_dropped_quantiles], lr=lr_dropped_quantiles)\n",
    "            self.first_training = True\n",
    "            self.diff_ma_coef = diff_ma_coef\n",
    "\n",
    "    def train(self, replay_buffer, batch_size=256, ptr_list=None, disc_return=None, do_beta_update=False):\n",
    "\n",
    "        if ptr_list is not None and do_beta_update:\n",
    "            self.update_beta(replay_buffer, ptr_list, disc_return)\n",
    "\n",
    "        for it in range(self.num_critic_updates):\n",
    "            state, action, next_state, reward, not_done = replay_buffer.sample(batch_size)\n",
    "            alpha = torch.exp(self.log_alpha)\n",
    "\n",
    "            # --- Q loss ---\n",
    "            with torch.no_grad():\n",
    "                # get policy action\n",
    "                new_next_action, next_log_pi = self.actor(next_state)\n",
    "                # compute and cut quantiles at the next state\n",
    "                next_z = self.critic_target(next_state, new_next_action)  # batch x nets x quantiles\n",
    "                sorted_z, _ = torch.sort(next_z.reshape(batch_size, -1))\n",
    "                if self.use_acc:\n",
    "                    sorted_z_part = sorted_z[:, :self.quantiles_total - round(self.critic.n_nets * self.adjusted_dropped_quantiles.item())]\n",
    "                else:\n",
    "                    sorted_z_part = sorted_z[:, :self.quantiles_total - self.top_quantiles_to_drop]\n",
    "\n",
    "                # compute target\n",
    "                target = reward + not_done * self.discount * (sorted_z_part - alpha * next_log_pi)\n",
    "\n",
    "            cur_z = self.critic(state, action)\n",
    "            critic_loss = quantile_huber_loss_f(cur_z, target.detach())\n",
    "\n",
    "            self.critic_optimizer.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            self.critic_optimizer.step()\n",
    "\n",
    "            for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
    "                    target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "\n",
    "\n",
    "        # --- Policy and alpha loss ---\n",
    "        new_action, log_pi = self.actor(state)\n",
    "        alpha_loss = -self.log_alpha * (log_pi + self.target_entropy).detach().mean()\n",
    "        actor_loss = (alpha * log_pi - self.critic(state, new_action).mean(2).mean(1, keepdim=True)).mean()\n",
    "\n",
    "\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        self.alpha_optimizer.zero_grad()\n",
    "        alpha_loss.backward()\n",
    "        self.alpha_optimizer.step()\n",
    "\n",
    "        self.total_it += 1\n",
    "\n",
    "    \n",
    "    def update_beta(self, replay_buffer, ptr_list=None, disc_return=None):\n",
    "        state, action = replay_buffer.states_by_ptr(ptr_list)\n",
    "        disc_return = torch.FloatTensor(disc_return).to(DEVICE)\n",
    "        assert disc_return.shape[0] == state.shape[0]\n",
    "\n",
    "        mean_Q_last_eps =  self.critic(state, action).mean(2).mean(1, keepdim=True).mean().detach()\n",
    "        mean_return_last_eps = torch.mean(disc_return).detach()\n",
    "\n",
    "        if self.first_training:\n",
    "            self.diff_mvavg = torch.abs(mean_return_last_eps - mean_Q_last_eps).detach()\n",
    "            self.first_training = False\n",
    "        else:\n",
    "            self.diff_mvavg = (1 - self.diff_ma_coef) * self.diff_mvavg \\\n",
    "                              + self.diff_ma_coef * torch.abs(mean_return_last_eps - mean_Q_last_eps).detach()\n",
    "\n",
    "        diff_qret = ((mean_return_last_eps - mean_Q_last_eps) / (self.diff_mvavg + 1e-8)).detach()\n",
    "        aux_loss = self.adjusted_dropped_quantiles * diff_qret\n",
    "        self.dropped_quantiles_dropped_optimizer.zero_grad()\n",
    "        aux_loss.backward()\n",
    "        self.dropped_quantiles_dropped_optimizer.step()\n",
    "        self.adjusted_dropped_quantiles.data = self.adjusted_dropped_quantiles.clamp(min=0., max=self.adjusted_dropped_quantiles_max)\n",
    "\n",
    "        # self.writer.add_scalar('learner/adjusted_dropped_quantiles', self.adjusted_dropped_quantiles, self.total_it)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args, prefix):\n",
    "    # --- Init ---\n",
    "    \n",
    "    log_f = open(\"agent-log.txt\",\"w+\")\n",
    "    \n",
    "\n",
    "    env = gym.make(args.env)\n",
    "    video_every = 25\n",
    "    env = gym.wrappers.Monitor(env, \"./videoACC\", video_callable=lambda ep_id: ep_id%video_every == 0, force=True)\n",
    "    \n",
    "    env.seed(seed)\n",
    "    env.action_space.seed(seed)\n",
    "\n",
    "    env = RescaleAction(env, -1., 1.)\n",
    "\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.shape[0]\n",
    "\n",
    "    replay_buffer = ReplayBuffer(state_dim, action_dim)\n",
    "    actor = Actor(state_dim, action_dim).to(DEVICE)\n",
    "    critic = Critic(state_dim, action_dim, args.n_quantiles, args.n_nets).to(DEVICE)\n",
    "    critic_target = copy.deepcopy(critic)\n",
    "\n",
    "    top_quantiles_to_drop = args.top_quantiles_to_drop_per_net * args.n_nets\n",
    "    \n",
    "    trainer = Trainer(actor=actor,\n",
    "                      critic=critic,\n",
    "                      critic_target=critic_target,\n",
    "                      top_quantiles_to_drop=top_quantiles_to_drop,\n",
    "                      discount=args.discount,\n",
    "                      tau=args.tau,\n",
    "                      target_entropy=-np.prod(env.action_space.shape).item(),\n",
    "                      use_acc=args.use_acc,\n",
    "                      lr_dropped_quantiles=args.lr_dropped_quantiles,\n",
    "                      adjusted_dropped_quantiles_init=args.adjusted_dropped_quantiles_init,\n",
    "                      adjusted_dropped_quantiles_max=args.adjusted_dropped_quantiles_max,\n",
    "                      diff_ma_coef=args.diff_ma_coef,\n",
    "                      num_critic_updates=args.num_critic_updates)\n",
    "\n",
    "    state, done = env.reset(), False\n",
    "    episode_return, last_episode_return = 0, 0\n",
    "    episode_timesteps = 0\n",
    "    episode_num = 0\n",
    "\n",
    "    actor.train()\n",
    "    for t in range(int(args.max_timesteps)):\n",
    "        \n",
    "        print(f\"T: {episode_timesteps}\", end=\"\\r\")\n",
    "        action = actor.select_action(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        episode_timesteps += 1\n",
    "        episode_return += reward\n",
    "   \n",
    "        replay_buffer.add(state, action, next_state, reward, done)\n",
    "\n",
    "        state = next_state\n",
    "        \n",
    "        # Train agent after collecting sufficient data\n",
    "        if t >= args.init_expl_steps:\n",
    "            trainer.train(replay_buffer, args.batch_size)\n",
    "\n",
    "\n",
    "        if done or episode_timesteps >= EPISODE_LENGTH:\n",
    "            print(f\"Total T: {t + 1} Episode Num: {episode_num + 1} Episode T: {episode_timesteps} Reward: {episode_return:.3f}\")\n",
    "            state, done = env.reset(), False\n",
    "            \n",
    "            log_f.write('episode: {}, reward: {}\\n'.format(episode_num, episode_return))\n",
    "            log_f.flush()\n",
    "\n",
    "            episode_return = 0\n",
    "            episode_timesteps = 0\n",
    "            episode_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sys.argv = [\n",
    "    \"notebook\",\n",
    "    \"--env\", \"BipedalWalkerHardcore-v3\",\n",
    "    \"--eval_freq\", \"5000\",\n",
    "    \"--max_timesteps\", \"1000000\",\n",
    "    \"--init_expl_steps\", \"5000\",\n",
    "    \"--seed\", \"42\",\n",
    "    \"--n_quantiles\", \"25\",\n",
    "    \"--use_acc\", \"True\",\n",
    "    \"--top_quantiles_to_drop_per_net\", \"2\",\n",
    "    \"--beta_udate_rate\", \"1000\",\n",
    "    \"--init_num_steps_before_beta_updates\", \"25000\",\n",
    "    \"--size_limit_beta_update_batch\", \"5000\",\n",
    "    \"--lr_dropped_quantiles\", \"0.1\",\n",
    "    \"--adjusted_dropped_quantiles_init\", \"2.5\",\n",
    "    \"--adjusted_dropped_quantiles_max\", \"5.0\",\n",
    "    \"--diff_ma_coef\", \"0.05\",\n",
    "    \"--num_critic_updates\", \"4\",\n",
    "    \"--n_nets\", \"5\",\n",
    "    \"--batch_size\", \"256\",\n",
    "    \"--discount\", \"0.99\",\n",
    "    \"--tau\", \"0.005\",\n",
    "    \"--log_dir\", \"results\",\n",
    "    \"--exp_name\", \"eval_run\",\n",
    "    \"--prefix\", \"\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "\n",
    "def str2bool(v):\n",
    "        if v.lower() in ('yes', 'true', 't', 'y', '1'):\n",
    "            return True\n",
    "        elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n",
    "            return False\n",
    "        else:\n",
    "            raise argparse.ArgumentTypeError('Boolean value expected.')\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--env\", default=\"BipedalWalkerHardcore-v3\")              # OpenAI gym environment name\n",
    "parser.add_argument(\"--eval_freq\", default=1e3, type=int)           # How often (time steps) we evaluate\n",
    "parser.add_argument(\"--max_timesteps\", default=1e6, type=int)       # Max time steps to run environment\n",
    "parser.add_argument(\"--init_expl_steps\", default=5000, type=int)    # num of exploration steps before training starts\n",
    "parser.add_argument(\"--seed\", default=42, type=int)                  # random seed\n",
    "parser.add_argument(\"--n_quantiles\", default=25, type=int)          # number of quantiles for TQC\n",
    "parser.add_argument(\"--use_acc\", default=True, type=str2bool)       # if acc for automatic tuning of beta shall be used, o/w top_quantiles_to_drop_per_net will be used\n",
    "parser.add_argument(\"--top_quantiles_to_drop_per_net\",\n",
    "                    default=3, type=int)        # how many quantiles to drop per net. Parameter has no effect if: use_acc = True\n",
    "parser.add_argument(\"--beta_udate_rate\", default=1000, type=int)# num of steps between beta/dropped_quantiles updates\n",
    "parser.add_argument(\"--init_num_steps_before_beta_updates\",\n",
    "                    default=25000, type=int)    # num steps before updates to dropped_quantiles are started\n",
    "parser.add_argument(\"--size_limit_beta_update_batch\",\n",
    "                    default=5000, type=int)     # size of most recent state-action pairs stored for dropped_quantiles updates\n",
    "parser.add_argument(\"--lr_dropped_quantiles\",\n",
    "                    default=0.1, type=float)    # learning rate for dropped_quantiles\n",
    "parser.add_argument(\"--adjusted_dropped_quantiles_init\",\n",
    "                    default=2.5, type=float)     # initial value of dropped_quantiles\n",
    "parser.add_argument(\"--adjusted_dropped_quantiles_max\",\n",
    "                    default=5.0, type=float)    # maximal value for dropped_quantiles\n",
    "parser.add_argument(\"--diff_ma_coef\", default=0.05, type=float)     # moving average param. for normalization of dropped_quantiles loss\n",
    "parser.add_argument(\"--num_critic_updates\", default=1, type=int)    # number of critic updates per environment step\n",
    "parser.add_argument(\"--n_nets\", default=5, type=int)                # number of critic networks\n",
    "parser.add_argument(\"--batch_size\", default=256, type=int)          # Batch size for both actor and critic\n",
    "parser.add_argument(\"--discount\", default=0.99, type=float)         # Discount factor\n",
    "parser.add_argument(\"--tau\", default=0.005, type=float)             # Target network update rate\n",
    "parser.add_argument(\"--log_dir\", default='results')                 # results directory\n",
    "parser.add_argument(\"--exp_name\", default='eval_run')               # name of experiment\n",
    "parser.add_argument(\"--prefix\", default='')                         # optional prefix to the name of the experiments\n",
    "args = parser.parse_args()\n",
    "\n",
    "log_dir = Path(args.log_dir)\n",
    "\n",
    "# Print the parsed arguments for confirmation\n",
    "print(f\"env: {args.env}\")\n",
    "print(f\"eval_freq: {args.eval_freq}\")\n",
    "print(f\"max_timesteps: {args.max_timesteps}\")\n",
    "print(f\"seed: {args.seed}\")\n",
    "print(f\"n_quantiles: {args.n_quantiles}\")\n",
    "print(f\"top_quantiles_to_drop_per_net: {args.top_quantiles_to_drop_per_net}\")\n",
    "print(f\"n_nets: {args.n_nets}\")\n",
    "print(f\"batch_size: {args.batch_size}\")\n",
    "print(f\"discount: {args.discount}\")\n",
    "print(f\"tau: {args.tau}\")\n",
    "print(f\"log_dir: {args.log_dir}\")\n",
    "print(f\"prefix: {args.prefix}\")\n",
    "\n",
    "# Call your main function using the parsed arguments\n",
    "main(args, args.prefix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
